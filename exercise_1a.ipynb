{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f7bedb-3295-4b93-87a8-8d0f831cba95",
   "metadata": {
    "id": "37f7bedb-3295-4b93-87a8-8d0f831cba95"
   },
   "source": [
    "---\n",
    "\n",
    "# Exercise 1a: Analysis of global temperature from 1850 to present\n",
    "\n",
    "## Introduction\n",
    "\n",
    "An important part of earth science is trying to look back into time to figure out what happened to our planet through geological history. For the more recent history earth science relies on historical measurements. However for going deeper into history earth science relies on extracting information from old sediments and rocks that are exposed at the surface, or buried below the surface and accessed through boreholes.\n",
    "\n",
    "The goal of this first part of this module is to get familiar with the analysis of timeseries data, which is data that describes how something has varied over time. We will work with data that informs us how the climate on our small planet has changed.\n",
    "\n",
    "Before we dive into deep geological history, we will first visualize and analyze more recent global temperature data that covers the period from the year 1850 to present. We will work with this data to see if we can identify trends and rhythms (periodic changes) in this dataset. Once we master this we will move on to exercise 1b where we will perform the same analysis, but then on climate data that spans the last 65 million years of the history of our planet.\n",
    "\n",
    "This exercise was inspired by a classical paper by James Zachos and co-authors called \"[Trends, Rhythms, and Aberrations in Global Climate 65 Ma to Present](https://doi.org/10.1126/science.1059412)\". These authors were one of the first to look at the long-term climate history of our planet, and tried to identify trends, rhythms and abberations in the global climate data. We will follow their lead, but use newer and more extensive data for our analysis.\n",
    "\n",
    "In this exercise you will learn more about earth science, in particular climate history, and you will obtain new skills in data analysis and programming.\n",
    "\n",
    "\n",
    "## How does this exercise work?\n",
    "\n",
    "This exercise is a [Jupyter](https://jupyter.org/) notebook. A Jupyter notebook contains a mix of text blocks and [Python](https://www.python.org/) code blocks that you can run yourself. The text blocks are used to explain the data analysis workflow of this exercise. The data analysis itself is performed in blocks of Python code. The Python code blocks contain a mix of actual code and comments, which start with `#` and which describe what each line of code below does.\n",
    "\n",
    "[Python](https://www.python.org/) is a programming language that is relatively easy to learn. A progamming language is a instruction language for a computer to make it do things that we find useful, such as analyzing climate data for us. Thanks to its accessibility Python has become the most popular programming language in science.\n",
    "\n",
    "We will run this notebook in [Google Colab](https://colab.research.google.com/). Google Colab gives us the opportunity to run this notebook online and in a web browser, which is the easiest way to complete this exercise.\n",
    "\n",
    "Note: If you prefer not to use Google then you can also run this notebook on your own computer. This requires installing Python and Jupyter on your own machine. The easiest to get things running is to install the Python distribution Anaconda: https://www.anaconda.com/download/ (notice the odd preference for snake names in the Python world). This may require a bit more fiddling, but your instructors for this exercise are happy to help out.\n",
    "\n",
    "## Working with this Jupyter notebook\n",
    "\n",
    "Before you start, make a copy of this notebook by selecting `File` and one of the `Save a copy...` options. This is to make sure you work on your own copy of the notebook.\n",
    "\n",
    "You can run each block of code by selecting the block and pressing the run symbol on the left of the block, or by pressing shift-enter. Or you can select `runtime` and then `Run all` to run all code blocks, `Run after` to run the selected code block and all blocks that come afterwards, or `Run before` to run all code blocks before the code block that you have selected.\n",
    "\n",
    "\n",
    "## Assignments and handing in this exercise\n",
    "\n",
    "This exercise contains a number of assignments, where you either have to complete a code block, or write some text in a text block. These assignments are marked clearly and numbered. You can hand in the assignments by modifying this notebook and handing in the completed notebook file on mitt.uib.\n",
    "\n",
    "Note that throughout this notebook we also provide links to papers or descriptions of data analysis methods or Python modules that you can read if you want to dive more into the earth science, data analysis and programming parts of this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88bf204-b13c-4b18-8cf3-f1990600cbce",
   "metadata": {
    "id": "d88bf204-b13c-4b18-8cf3-f1990600cbce"
   },
   "source": [
    "----\n",
    "\n",
    "# Section 1: Global historic temperature dataset <a name=\"data\"></a>\n",
    "\n",
    "## 1.1 Getting the dataset\n",
    "We will start by obtaining a dataset of the global average temperature from 1850 to present from https://berkeleyearth.org/data/. We will then use this dataset for our analyses. The dataset by going to the Berkeley Earth website:\n",
    "You can download the data here: [Land_and_Ocean_complete_mod.csv](https://raw.githubusercontent.com/ElcoLuijendijk/intro_earth_science_informatics/main/data/Land_and_Ocean_complete_mod.csv), which you can inspect using a text editor or Excel.\n",
    "\n",
    "If you would like to learn more about the processing of the data to estimate a global average temperature out of different data sources you can read this paper  by Rohde and Hausfather (2020): https://doi.org/10.5194/essd-12-3469-2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e778a02-21a4-4470-98fa-d7c8434a0fc2",
   "metadata": {
    "id": "2e778a02-21a4-4470-98fa-d7c8434a0fc2"
   },
   "source": [
    "----\n",
    "\n",
    "# Section 2: A first look at the global temperature data\n",
    "\n",
    "We will analyze the temperature data with the programming language Python (https://www.python.org/). We will explain all the code that is used for the analysis step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4554d23-3fff-4f8a-87e9-55c927431013",
   "metadata": {
    "id": "e4554d23-3fff-4f8a-87e9-55c927431013"
   },
   "source": [
    "## 2.1 Importing additional modules\n",
    "\n",
    "First we start by adding some additional modules of code that we need for the analysis. Python already has a lot of capabilities for analysis, but the programming language can be extended with additional modules for things like making nice figures, working with alrge datasets and better statistical analysis.\n",
    "\n",
    "Below we add three such modules, Numpy (https://numpy.org/), Pandas (https://pandas.pydata.org/) and Matplotlib (https://matplotlib.org/). Numpy is used for additional computation with large sets of data, Pandas makes reading, writing and working with large datasets much easier, and Matplotlib is great for making nice looking figures or animations (follow this link for some nice examples: https://matplotlib.org/stable/plot_types/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5464ec-3c0d-488d-885b-09f3a802f692",
   "metadata": {
    "id": "fe5464ec-3c0d-488d-885b-09f3a802f692"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eff78fc-1476-4590-9f40-ed941a80dc5d",
   "metadata": {
    "id": "3eff78fc-1476-4590-9f40-ed941a80dc5d"
   },
   "source": [
    "## 2.2 Reading the global historic temperature datafile\n",
    "\n",
    "Next we will read the temperature datafile and display a summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec3850-edbd-4716-a4cd-7af9c5f07f0f",
   "metadata": {
    "id": "14ec3850-edbd-4716-a4cd-7af9c5f07f0f"
   },
   "outputs": [],
   "source": [
    "# here we specify the name of the datafile.\n",
    "filename = \"https://raw.githubusercontent.com/ElcoLuijendijk/intro_earth_science_informatics/main/data/Land_and_Ocean_complete_mod.csv\"\n",
    "\n",
    "# load the datafile into an object called df (which is short for dataframe)\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265204dd-ad7a-4e81-b1fd-341d8d87832d",
   "metadata": {
    "id": "265204dd-ad7a-4e81-b1fd-341d8d87832d"
   },
   "source": [
    "We can now show the contents of the datafile by simply typing the name of the dataset (``df``) in a code block, as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e813215-48ca-4983-af31-cc3a2f764712",
   "metadata": {
    "id": "1e813215-48ca-4983-af31-cc3a2f764712"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qam2soilrNXx",
   "metadata": {
    "id": "Qam2soilrNXx"
   },
   "source": [
    "One way to have a quick summary of the contents of this datafile is the `.describe()` command, which will show you statistics for each column in the datafile as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KhvAkjtirI2k",
   "metadata": {
    "id": "KhvAkjtirI2k"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466e289-2e1e-4093-929f-c4858af828b7",
   "metadata": {
    "id": "f466e289-2e1e-4093-929f-c4858af828b7"
   },
   "source": [
    "----\n",
    "\n",
    "# Section 3: Visualizing the data <a name=\"visualizing\"></a>\n",
    "\n",
    "Next we will visualize the climate data using Python's nice visualization module [Matplotlib](https://matplotlib.org/). We will create a figure with one panel, and we will plot the temperature anomaly in this panel using the Python code in the next code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692d190-303b-443d-ba60-8b509892952e",
   "metadata": {
    "id": "3692d190-303b-443d-ba60-8b509892952e"
   },
   "outputs": [],
   "source": [
    "# make a new figure with one subplot. The figure is called fig, and the subplot is called ax\n",
    "# note that all commands that use matplotlib start with pl.\n",
    "# because we imported matplotlib's pyplot module as pl earlier\n",
    "fig, ax = pl.subplots(1, 1)\n",
    "\n",
    "# plot the monthly temperature anomaly\n",
    "ax.plot(df[\"monthly_anomaly\"], color=\"tab:blue\", linewidth=0.5)\n",
    "\n",
    "# add labels to the x and y axes\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Temperature anomaly (degr. C)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567eba1a-6d7c-4d5b-9484-1da2135bda79",
   "metadata": {
    "id": "567eba1a-6d7c-4d5b-9484-1da2135bda79"
   },
   "source": [
    "## 3.1 Improving the global temperature anomaly figure\n",
    "\n",
    "This figure above is nice, but we can do better of course. We first would like to have real dates or years on the x-axis. Thankfully the Pandas module makes this easy and has a command ``date_range``  (see [this link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html) for the optional extensive description of what this command can do) that helps us to add column with dates to the dataset. Execute the code block below to create an additional column called `date` that has a single value for each datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272fc97-c369-4768-88f1-6adcae720a0b",
   "metadata": {
    "id": "a272fc97-c369-4768-88f1-6adcae720a0b"
   },
   "outputs": [],
   "source": [
    "# detemrine the number of months in the entire dataset\n",
    "n_months = len(df)\n",
    "print(f\"There are {n_months} months in the dataset\")\n",
    "\n",
    "# find first year and the first months:\n",
    "# the first row of a dataframe can be called by df.iloc[0]\n",
    "first_year = int(df.iloc[0][\"year\"])\n",
    "first_month = int(df.iloc[0][\"month\"])\n",
    "\n",
    "print(f\"The first year is {first_year}, and the first month is {first_month}\")\n",
    "\n",
    "# create a date range from the first month to the last\n",
    "dti = pd.date_range(f\"{first_year}-{first_month}-01\", periods=n_months, freq=\"MS\")\n",
    "\n",
    "print(\"The date range in the datafile is :\", dti)\n",
    "\n",
    "# add a column with this date range to the dataframe\n",
    "print(\"adding this date range to the dataframe\")\n",
    "df[\"date\"] = dti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WUnQpCXzqrMs",
   "metadata": {
    "id": "WUnQpCXzqrMs"
   },
   "source": [
    "Next we display the contents of the dataset again. If all went well there should be a column called `date` on the right hand side (scroll to the right after running the next code block to see it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af92f1-5bc6-4239-85a5-15d4454ea8ca",
   "metadata": {
    "id": "f4af92f1-5bc6-4239-85a5-15d4454ea8ca"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc4d28c-38f4-4847-81c6-bcebeb9bb837",
   "metadata": {
    "id": "8cc4d28c-38f4-4847-81c6-bcebeb9bb837"
   },
   "source": [
    "Now we are ready to make a nicer figure with dates on the x-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7352ecb9-078b-4f87-b277-218978b8d0c6",
   "metadata": {
    "id": "7352ecb9-078b-4f87-b277-218978b8d0c6"
   },
   "outputs": [],
   "source": [
    "# make a new figure with one subplot. The figure is called fig, and the subplot is called ax\n",
    "# note that all commands that use matplotlib start with pl.\n",
    "# because we imported matplotlib's pyplot module as pl earlier\n",
    "fig, ax = pl.subplots(1, 1)\n",
    "\n",
    "# plot the monthly temperature anomaly, this time using real dates on the x-axis\n",
    "ax.plot(df[\"date\"], df[\"monthly_anomaly\"], color=\"tab:blue\", linewidth=0.5)\n",
    "\n",
    "# add a horizontal line at y=0\n",
    "ax.axhline(y=0, color=\"black\", lw=0.5)\n",
    "\n",
    "# add labels to the x and y axes\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Temperature anomaly (degr. C)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9226fa-56cd-4b97-812e-0e22aea20222",
   "metadata": {
    "id": "8f9226fa-56cd-4b97-812e-0e22aea20222"
   },
   "source": [
    "----\n",
    "\n",
    "# Section 4: Analysing trends in the global temperature data <a name=\"analysis\"></a>\n",
    "\n",
    "The temperature curve shows a strong increase in global temperature over the last decades.\n",
    "\n",
    "A natural question to ask is how fast does temperature increase? Can we quantify this?\n",
    "\n",
    "In the next part of this exercise we will try to fit a number of simple models to the data to see how well they can explain the temperatures on our planet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842e12c-3f98-4c14-af54-a7bde2378526",
   "metadata": {
    "id": "9842e12c-3f98-4c14-af54-a7bde2378526"
   },
   "source": [
    "## 4.1 Linear regression\n",
    "\n",
    "The first approach that we will follow here to analyse the data is applying a linear regression to the dataset. A linear regression tries to fit a line to the data that follows a simple equation in the shape of:\n",
    "\n",
    "$$y = ax + b$$\n",
    "\n",
    "In this case the variable $y$ is the temperature anomaly and variable $x$ would be time. Variables $a$ and $b$ determine the slope and intercept of the regression line. In a linear regression these two parameters are fitted so that the line runs through the data as close as possible. We will use a built in [linear regression function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html) from the Python module [scipy](https://scipy.org/) in the code block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79456913-855d-4006-9c07-c3da2afe52ac",
   "metadata": {
    "id": "79456913-855d-4006-9c07-c3da2afe52ac"
   },
   "outputs": [],
   "source": [
    "# import the scipy module that contain the linear regression function\n",
    "import scipy.stats as st\n",
    "\n",
    "# generate a new column with the month number\n",
    "df[\"month_number\"] = np.arange(len(df))\n",
    "\n",
    "# assigning the x and y variables for the regression\n",
    "x = df[\"month_number\"].values\n",
    "y = df[\"monthly_anomaly\"].values\n",
    "\n",
    "# perform the regression result\n",
    "regression_result = st.linregress(x, y)\n",
    "\n",
    "# print the results to screen\n",
    "print(\"The result of the linear regression between temperature and time is: \", regression_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de8a4d-f881-44fd-8888-fa79484c4b48",
   "metadata": {},
   "source": [
    "### Accessing the results of the linear regression\n",
    "\n",
    "You can access the results of the linear regression by typing the following commands in an empty new code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e34eb-6059-4b4d-b960-bcc5ee924c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_result.slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe1c34-a421-459b-a7ec-30972e22e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_result.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a5663-5a5d-4f28-aa98-f787e4b57583",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_result.rvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c32414e-d60c-4a58-b4bb-73d56689e932",
   "metadata": {},
   "source": [
    "You can also do maths with these values, as shown below, where we calculate a new variable, which we call `test_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784a76f-e0d3-4fe2-a25f-af4fe916d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value = regression_result.slope * 2\n",
    "test_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d185f294-71c0-4edd-8c4d-7cc17af69397",
   "metadata": {},
   "source": [
    "For a review of how to calculate things in Python, see this website: [https://en.wikibooks.org/wiki/Python_Programming/Basic_Math](https://en.wikibooks.org/wiki/Python_Programming/Basic_Math), or look at your notes from the Python workshop. In short, adding things is done with +, multiplication using a star \\*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4035c-dc6a-4f53-9f4d-d3dbab93867e",
   "metadata": {
    "id": "fee4035c-dc6a-4f53-9f4d-d3dbab93867e"
   },
   "source": [
    "### Plotting the regression line\n",
    "\n",
    "To make the results of the linear regression more visual we will make a figure where the regression line and the dataset are shown together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76820ef0-dcf7-4ba4-97a5-be9a27f997f7",
   "metadata": {
    "id": "76820ef0-dcf7-4ba4-97a5-be9a27f997f7"
   },
   "outputs": [],
   "source": [
    "# here we calculate the linear regression by multiplying the month number with the slope and adding the intercept:\n",
    "df[\"temperature_linear_model\"] = x * regression_result.slope + regression_result.intercept\n",
    "\n",
    "# make a label for the figure legend\n",
    "linregress_label = f\"linear regression, y={regression_result.intercept:0.2f} + {regression_result.slope:0.2e} x \\n\"\n",
    "linregress_label += fr\"$R^2$={regression_result.rvalue**2:0.3f}, p={regression_result.pvalue:0.3f}\"\n",
    "\n",
    "# make a new figure with one subplot. The figure is called fig, and the subplot is called ax\n",
    "# note that all commands that use matplotlib start with pl.\n",
    "# because we imported matplotlib's pyplot module as pl earlier\n",
    "fig, ax = pl.subplots(1, 1)\n",
    "\n",
    "# plot the monthly temperature anomaly, this time using real dates on the x-axis\n",
    "ax.plot(df[\"date\"], df[\"monthly_anomaly\"], color=\"tab:blue\", linewidth=0.5, label=\"global temperature anomaly\")\n",
    "\n",
    "# plot the linear regression\n",
    "ax.plot(df[\"date\"], df[\"temperature_linear_model\"], color=\"tab:orange\", linewidth=1.5, label=linregress_label)\n",
    "\n",
    "# add a horizontal line at y=0\n",
    "ax.axhline(y=0, color=\"black\", lw=0.5)\n",
    "\n",
    "# add a legend\n",
    "ax.legend()\n",
    "\n",
    "# add labels to the x and y axes\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Temperature anomaly (degr. C)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18241ef0-1762-447e-894d-675968c11a1c",
   "metadata": {
    "id": "18241ef0-1762-447e-894d-675968c11a1c"
   },
   "source": [
    "### Evaluating the regression model\n",
    "\n",
    "\n",
    "The regression line above provides a reasonable, but not perfect fit to the data. The regression algorithm itself includes two metrics that we can use to judge how well this model fits the global temperature data:\n",
    "\n",
    "**p value and the null hypothesis**\n",
    "The variable `p` in the figure above denotes the p-value, which is the probability of the [null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis). The null hypothesis is the hypothesis that the effect that is being studied does not exist. In this case the null hypothesis tests whether or not the linear regression explains the data. A probability of 1 would show that the null hypothesis is true, i.e. there is no linear correlation between time and temperature. If the probability is 0.0 there is no probability that the null hypothesis is true. In general a value of `p` that is less than 0.05 is accepted.\n",
    "\n",
    "\n",
    "**The coefficient of determination $R^2$**\n",
    "The p-value provides information on whether or not there is a relation between the model that we are testing and the data, but it does not tell us how strong the relation is. Does it explain all of the data or are there still some unexplained trends or noise in the data? For this we need to look at a different metric, the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination). A simple definition of the coefficient of determination is:\n",
    "\n",
    "$$R^2 = 1 - \\dfrac{\\text{unexplained variance}}{\\text{variance of the dataset}}$$\n",
    "\n",
    "As you can see from this definition the value of $R^2$ goes to 1.0 if all of the variance of the data can be explained by the model. And if the unexplained variance is equal to the variance of the dataset, the model is actually useless and $R^2$ equals 0.0. Getting a model prediction close to an $R^2$ of 1.0 is the goal of most models, but is rarely feasible in the earth sciences because there are often many different processes that affect geoscientific data and constructing a model that represents all processes well is often difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b0fa0-6f57-418d-82ce-6fe4a0ab471b",
   "metadata": {
    "id": "d21b0fa0-6f57-418d-82ce-6fe4a0ab471b"
   },
   "source": [
    "---\n",
    "# *Assigment 1: Evaluate the linear regression model*\n",
    "\n",
    "**Based on the explanation above, write a few lines on how the regression model performs in your opinion. Write this in the text block below:**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d79a0-dbf1-4f92-884d-29790e7a5f30",
   "metadata": {
    "id": "923d79a0-dbf1-4f92-884d-29790e7a5f30"
   },
   "source": [
    "*Add your answer to assignment 1 here....*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a367daac-c7e4-4d61-b43f-06b6909ec2bd",
   "metadata": {
    "id": "a367daac-c7e4-4d61-b43f-06b6909ec2bd"
   },
   "source": [
    "---\n",
    "\n",
    "# *Assignment 2: Predict future global temperature*\n",
    "\n",
    "The figure that we made earlier in this motebook is a nice way to communicate what is going on with global climate. But it would also be interesting to communicate some simple metrics like, on average the temperature increases by x per year, or if current trends continue the global temperature will be ... degrees C.\n",
    "\n",
    "Your next task for this assignment is to complete the code below to calculate the average temperature increase per year, and the projected temperature in the year 2050.\n",
    "\n",
    "*Hint: Use the results of the linear regression that you completed in section 4.1 for the assignment below. `regression_result.slope` gives you the slope of the regression line, in degrees C per month, and `regression_result.intercept` gives you the temperature at the start of the temperature dataset, i.e., january 1850. For a review of how to calculate things in Python, see this website: [https://en.wikibooks.org/wiki/Python_Programming/Basic_Math](https://en.wikibooks.org/wiki/Python_Programming/Basic_Math), or look at your notes from the Python workshop. In short, adding things is done with +, multiplication using a star \\*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961b2fb-5790-486a-b0ea-3d73948bd9d0",
   "metadata": {
    "id": "d961b2fb-5790-486a-b0ea-3d73948bd9d0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# complete the line below and remove the comment mark # at the start of the line:\n",
    "annual_temp_increase = .....\n",
    "\n",
    "print(f\"The average increase in global temperature per year equals {annual_temp_increase} degrees C per year\")\n",
    "\n",
    "# complete the lines below to calculate the temperatures in the year 2050 and 2100:\n",
    "temperature_in_2050 = .....\n",
    "temperature_in_2100 = .....\n",
    "\n",
    "print(f\"The expected global temperature in the year 2050 is {temperature_in_2050} degrees C higher than the baseline\")\n",
    "print(f\"and the expected global temperature in the year 2100 is {temperature_in_2100} degrees C higher than the baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a362b-da77-4810-b6d7-b3a6835ee1f7",
   "metadata": {
    "id": "107a362b-da77-4810-b6d7-b3a6835ee1f7"
   },
   "source": [
    "----\n",
    "\n",
    "# Section 5: Finding a better model <a name=\"better_analysis\"></a>\n",
    "\n",
    "You may notice that the above linear regression curve does not fit the data that well. The increase in the last few decades seems faster than is indicated by the regression line, i.e. the blue curve that shows the data lies above the orange curve on the right hand side of the graph. Let's try to find a better model that can explain the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302abbf-738b-4851-8346-931242dc6795",
   "metadata": {
    "id": "e302abbf-738b-4851-8346-931242dc6795"
   },
   "source": [
    "## 5.1 Piecewise linear model\n",
    "\n",
    "We will first try a piecewise linear model. In contrast to a linear regression model, the piecewise linear model allows for several lines with different slopes. In principle it would not be difficult to code this ourselves. However to keep things easy, we will use a Python module that was especially made to work with piecewise linear models, called [pwlf](https://github.com/cjekel/piecewise_linear_fit_py).\n",
    "\n",
    "We first need to install the pwlf module, which is done with the line of code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y9RyhoH2G8hM",
   "metadata": {
    "id": "y9RyhoH2G8hM"
   },
   "outputs": [],
   "source": [
    "%pip install pwlf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dP7d5rn4FX",
   "metadata": {
    "id": "71dP7d5rn4FX"
   },
   "source": [
    "If all went well you should have gotten a bunch of messages above, including the message `Successfully installed pwlf...`\n",
    "\n",
    "Now that we have installed pwlf, let's see what it can do. In the code block below we will use pwlf to fit a piecewise linear model with two segments to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d30bc4-8a7f-485e-9365-a8a9530893ee",
   "metadata": {
    "id": "81d30bc4-8a7f-485e-9365-a8a9530893ee"
   },
   "outputs": [],
   "source": [
    "# https://github.com/cjekel/piecewise_linear_fit_py\n",
    "# https://github.com/cjekel/piecewise_linear_fit_py/blob/master/examples/fitForSpecifiedNumberOfLineSegments.py\n",
    "\n",
    "# import the piecewise linear function module\n",
    "import pwlf\n",
    "\n",
    "# set up the model\n",
    "my_pwlf = pwlf.PiecewiseLinFit(x, y)\n",
    "\n",
    "# fit the model to the data\n",
    "# change the number between brackets to change the number of line segments\n",
    "results_pwlf = my_pwlf.fit(2)\n",
    "\n",
    "# get the results\n",
    "#b, a = results_pwlf\n",
    "\n",
    "# get the predicted values\n",
    "df[\"temperature_piecewise_linear_model\"] = my_pwlf.predict(x)\n",
    "\n",
    "print(f\"The parameters for the fitted piecewise linear model are {my_pwlf.beta}\")\n",
    "\n",
    "# calculate the coefficient of determination (R2)\n",
    "import sklearn.metrics\n",
    "R2_pwlf = sklearn.metrics.r2_score(y, df[\"temperature_piecewise_linear_model\"])\n",
    "print(f\"the R2 score for the piecewise linear model is {R2_pwlf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796a2b9-3859-4417-80e6-f3542ee2938e",
   "metadata": {
    "id": "0796a2b9-3859-4417-80e6-f3542ee2938e"
   },
   "source": [
    "\n",
    "Ok, now we have fitted the piecewise linear model to the global temperature data. However, the code above did not give us much information on what it did. To figure out what Python has done we still need to visualize the results and make a new figure.\n",
    "\n",
    "Since we will make similar figures a few time in this notebook we will first write a [*function*](https://docs.python.org/3/tutorial/controlflow.html#defining-functions) for the visualization of the results.\n",
    "\n",
    "Think of a Python function as a recipe in a cookbook. Just like a recipe outlines the steps to create a cake, a function defines a set of instructions to perform a specific task. You can call a function whenever you need to use those instructions and you donâ€™t want to write them out step by step every time, similar to how you refer to a recipe when you want to bake a cake. Functions can also take ingredients (called parameters) and return a finished cake (the result). For example, if you bake a cake with apples, you will get an apple pie, but if you would use pears, you will get a pear pie, even though you follow the same steps. This is also how a function works. Depending on the parameters you put in, the output differs. Therefore you can keep using the function. This makes your code organized and reusable, much like how a cookbook helps you recreate your favorite dishes anytime.\n",
    "\n",
    "We will first run the first code block below to create the function, and then run the second code block to visualize the new model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6deb25e-38b2-4db1-9af5-846655ee0207",
   "metadata": {
    "id": "a6deb25e-38b2-4db1-9af5-846655ee0207"
   },
   "outputs": [],
   "source": [
    "# the code below creates the function to visualize the results\n",
    "def temperature_figure(date, y_values, labels, lws=[0.5, 1.0, 1.0, 1.0, 1.0, 1.0]):\n",
    "    \"\"\"\n",
    "    This is a function to create a figure\n",
    "\n",
    "    defining this as a function has the advantage that this code can be reused easily later on in this notebook\n",
    "    \"\"\"\n",
    "\n",
    "    # get a color palette for the figure\n",
    "    colors = pl.cm.tab10.colors\n",
    "\n",
    "    # make a new figure with one subplot. The figure is called fig, and the subplot is called ax\n",
    "    # note that all commands that use matplotlib start with pl.\n",
    "    # because we imported matplotlib's pyplot module as pl earlier\n",
    "    fig, ax = pl.subplots(1, 1)\n",
    "\n",
    "    # go through all y_values passed to this function and plot each of these\n",
    "    for y_value, color, label, lw in zip(y_values, colors, labels, lws):\n",
    "        ax.plot(date, y_value, color=color, linewidth=lw, label=label)\n",
    "\n",
    "    # add a horizontal line at y=0\n",
    "    ax.axhline(y=0, color=\"black\", lw=0.5)\n",
    "\n",
    "    # add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    # add labels to the x and y axes\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Temperature anomaly (degr. C)\")\n",
    "\n",
    "    # return the figure and subplot to the main code:\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ff973-a697-4d79-b15a-6f2bb24ea81c",
   "metadata": {
    "id": "3d1ff973-a697-4d79-b15a-6f2bb24ea81c"
   },
   "outputs": [],
   "source": [
    "# run the function to create a figure\n",
    "temperature_figure(df[\"date\"], [df[\"monthly_anomaly\"], df[\"temperature_linear_model\"], df[\"temperature_piecewise_linear_model\"]],\n",
    " [\"data\", \"linear regression\", \"piecewise linear function\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51443e09-7f47-4983-8e51-e3e7aaaa9ff7",
   "metadata": {
    "id": "51443e09-7f47-4983-8e51-e3e7aaaa9ff7"
   },
   "source": [
    "## 5.2 Alternative models\n",
    "\n",
    "The piecewise linear model arguably does a much better job at fitting the data as you can see in the figure above. However, the piecewise linear model is not the only possible model that can explain the data. Another model that would allow for a stronger increase in temperatures over the last few decades is an powerlaw function. We will try to fit a [powerlaw](https://en.wikipedia.org/wiki/Power_law) function with the shape of:\n",
    "\n",
    "$$ y = a x^b + c$$\n",
    "\n",
    "For the linear regression model and the piecewise linear model we used specialized Python functions and libraries. However we can also use a more general approach using a [scipy](https://scipy.org/) command called [curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html). With the `curve_fit` command we can fit any equation or model to the data.\n",
    "\n",
    "Curve fit works by fitting a model that is contained in a separate Python function. The function `powerlaw_model` in the code block below takes in the variable `x` and an optional number of other variables (for instance `a`, `b` and `c`) and returns the predicted `y` value. In our case `x` represents time, in number of months since the start of the records, and `y_predicted` is the temperature predicted by the model.\n",
    "\n",
    "Run the code below to see if we can fit an powerlaw model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a8dcab-0a7f-4e7b-a22a-7269a7a7a8ab",
   "metadata": {
    "id": "d6a8dcab-0a7f-4e7b-a22a-7269a7a7a8ab"
   },
   "outputs": [],
   "source": [
    "# import the curve_fit command from the scipy module\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# the next lines of code are a function that contains the exponential model.\n",
    "# The function takes in the variables x, a, b and c and returns the variable y_predicted\n",
    "def powerlaw_model(x, a, b, c):\n",
    "\n",
    "    #y_predicted = a**(b*x) + c\n",
    "    y_predicted = a*x**b + c\n",
    "\n",
    "    return y_predicted\n",
    "\n",
    "# define the starting values for the parameters. Curve_fit will find the parameter values that best fit the data but needs reasonable first estimates to get going:\n",
    "starting_parameters = [1e-3, 1e-2, -0.5]\n",
    "\n",
    "# fit the model to the data\n",
    "results_pl = curve_fit(powerlaw_model, x, y, starting_parameters)\n",
    "\n",
    "print(f\"Fitted parameters for the powerlaw model: a = {results_pl[0][0]}, b={results_pl[0][1]}, c={results_pl[0][2]}\")\n",
    "\n",
    "# get the fitted parameter values\n",
    "a_pl, b_pl, c_pl = results_pl[0]\n",
    "\n",
    "# calculate the predicted values of y\n",
    "df[\"temperature_powerlaw_model\"] = powerlaw_model(x, a_pl, b_pl, c_pl)\n",
    "\n",
    "R2_pl = sklearn.metrics.r2_score(y, df[\"temperature_powerlaw_model\"])\n",
    "\n",
    "print(f\"R2 powerlaw model = {R2_pl}\")\n",
    "\n",
    "# create a figure\n",
    "temperature_figure(df[\"date\"],\n",
    " [df[\"monthly_anomaly\"],\n",
    "  df[\"temperature_linear_model\"],\n",
    "  df[\"temperature_piecewise_linear_model\"],\n",
    "  df[\"temperature_powerlaw_model\"]],\n",
    " [\"data\", \"linear regression\", \"piecewise linear function\", \"powerlaw\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda99fec-2765-473e-9c3b-118f87ac990d",
   "metadata": {
    "id": "dda99fec-2765-473e-9c3b-118f87ac990d"
   },
   "source": [
    "---\n",
    "\n",
    "# Section 6: Which models are good and which is best?\n",
    "\n",
    "We now have several different models to quantify the long-term trend in the global temperature data. Each model has a different $R^2$ value. At first sight we might be tempted to simply select the model with the highest $R^2$ value as the best model. However this is not entirely a fair way of selecting the best model. Imagine if we had used the piecewise linear model, but had increased the amount of segments to 1000. After a lot of number crunching the algorithm would come up with a near perfect model, where almost every temperature datapoint has its own line segment that overlies each datapoint. This model will obviously have a very high $R^2$ that would be close to 1.0, because it fits the data perfectly. It however also requires a 1000 input parameter values. And its value in predicting future warming may be very limited, because these 1000 parameters are exactly tuned to the data but may not capture the processes that cause the warming very well. Such a model is a case of a model that is *overfitted* (https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "Form a science philosophy viewpoint, scientific models and theories should follow Occam's razor (https://en.wikipedia.org/wiki/Occam%27s_razor), which is usually summarized as \"The simplest explanation is usually the best one\". Or in our case, the model with the least unnecessary assumptions is the best one. Therefore, we should in principle select the simplest model that fits the data well, i.e. a good model with a low number of parameters. However this raises a question which model should be rated best, the simple linear regression with just two parameters, but a poor fit to the data, or a more complex model with more parameters, but a better fit?\n",
    "\n",
    "To avoid overfitting we can resort to alternative metrics that take into account the complexity of a model. We will use two metrics here, the [Akaike information criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) or AIC and the [Bayesian information criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion) or BIC. Both AIC and BIC take into account the model error or residual, i.e., how far off the model is from the data, and the number of parameters that a model requires.\n",
    "\n",
    "The Akaike information criterion is calculated as follows:\n",
    "\n",
    "$$AIC = N \\ln \\left( \\dfrac{RSS}{N} \\right)+ 2 K$$\n",
    "\n",
    "where $N$ is the number of observations or datapoints, $K$ is the number of parameters and $RSS$ is the sum of the squared residuals, which is calculated by comparing each datapoint to the model prediction, squaring this and then summing all squared residuals:\n",
    "\n",
    "$$RSS = \\sum{(y_p - y)^2}$$\n",
    "\n",
    "Where $y_p$ is the value predicted by the model and $y$ is the actual data. The Bayesian information criterion is calculated in a similar way, but with a slightly different weighing of the importance of the number of parameters vs the model error:\n",
    "\n",
    "$$BIC = N \\ln \\left( \\dfrac{RSS}{N} \\right)+ \\sqrt{N} K$$\n",
    "\n",
    "The BIC metric tends to assign a higher importance to the number of parameters (K) in most cases, because $\\sqrt{N}$ is usually higher than 2. For both metrics the model that has the lowest AIC or BIC score is the best, because both a high model error (RSS) or a high number of parameters (K) increase the score.\n",
    "\n",
    "Feel free to use either of these two metrics in the rest of this exercise. There is an ongoing debate in the statistics community on which one is better, but for now it seems a matter of personal preference. If you want to read more about this you can check [this paper by Ding et al. (2018)](https://ieeexplore.ieee.org/abstract/document/8498082) (which is addmittedly not the easiest paper to read)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96bf8a5-2576-4c1c-985b-a39737896d63",
   "metadata": {
    "id": "b96bf8a5-2576-4c1c-985b-a39737896d63"
   },
   "source": [
    "---\n",
    "\n",
    "# *Assignment 3*\n",
    "\n",
    "Let's see how our model compare with these two new metrics. **First, complete the code below to test all the models that you have constructed so far:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2413539-3e9b-4ec7-a718-2b8f4b02d19a",
   "metadata": {
    "id": "b2413539-3e9b-4ec7-a718-2b8f4b02d19a"
   },
   "outputs": [],
   "source": [
    "def AIC(data, prediction, number_of_params):\n",
    "\n",
    "    N = len(data)\n",
    "\n",
    "    RSS = np.sum((data - prediction)**2)\n",
    "\n",
    "    K = number_of_params\n",
    "\n",
    "    AIC= N * np.log(RSS / N) + 2 * K\n",
    "\n",
    "    return AIC\n",
    "\n",
    "\n",
    "def BIC(data, prediction, number_of_params):\n",
    "\n",
    "    N = len(data)\n",
    "\n",
    "    RSS = np.sum((data - prediction)**2)\n",
    "\n",
    "    K = number_of_params\n",
    "\n",
    "    BIC= N * np.log(RSS / N) + np.log(N) * K\n",
    "\n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811dd164-1251-4010-bb9e-07eaa09a773a",
   "metadata": {
    "id": "811dd164-1251-4010-bb9e-07eaa09a773a"
   },
   "outputs": [],
   "source": [
    "AIC_linear = AIC(df[\"monthly_anomaly\"], df[\"temperature_linear_model\"], 2)\n",
    "BIC_linear = BIC(df[\"monthly_anomaly\"], df[\"temperature_linear_model\"], 2)\n",
    "print(f\"AIC for the linear regression: {AIC_linear}\")\n",
    "print(f\"BIC for the linear regression: {BIC_linear}\")\n",
    "\n",
    "AIC_pwlf = AIC(df[\"monthly_anomaly\"], df[\"temperature_piecewise_linear_model\"], 3)\n",
    "BIC_pwlf = BIC(df[\"monthly_anomaly\"], df[\"temperature_piecewise_linear_model\"], 3)\n",
    "print(f\"AIC for the piecewise linear model: {AIC_pwlf}\")\n",
    "print(f\"BIC for the piecewise linear model: {BIC_pwlf}\")\n",
    "\n",
    "# add code to evaluate the other models that you have constructed below:\n",
    "# AIC... = ....\n",
    "# BIC... = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b76e70-f7a0-4b0d-b265-c63db49bbb46",
   "metadata": {
    "id": "76b76e70-f7a0-4b0d-b265-c63db49bbb46"
   },
   "source": [
    "# *Assignment 3 continued*:\n",
    "\n",
    "**Second, write a few sentences on which model you would have chosen 1) subjectively, without looking at metrics, 2) based on $R^2$ and 3) based on AIC and/or BIC and 4) your final favorite model. Note: there is no right or wrong here, any choice that is well motivated will be accepted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd0c7a-a793-490b-a817-7fb735906999",
   "metadata": {
    "id": "1cbd0c7a-a793-490b-a817-7fb735906999"
   },
   "source": [
    "*Add your answer to here....*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yIrgoxVQL40t",
   "metadata": {
    "id": "yIrgoxVQL40t"
   },
   "source": [
    "---\n",
    "\n",
    "# *Assignment 4: Update your prediction of global temperature*\n",
    "\n",
    "The different models also generate different predictions of the temperature increase in the future. **Complete the code below to predict the global temperature in the year 2050 and 2100:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zKvYKJgKOe_8",
   "metadata": {
    "id": "zKvYKJgKOe_8"
   },
   "outputs": [],
   "source": [
    "# complete the lines below to calculate the temperatures in the year 2050 and 2100:\n",
    "# use the temperatures predicted by your favorite model\n",
    "\n",
    "# uncomment one of these lines to calculate the temperature in 2050\n",
    "# replace the number_of_months with the actual number of months since january 1850\n",
    "#temperature_in_2050_updated = my_pwlf.predict(number_of_months)\n",
    "#temperature_in_2050_updated = better_model(number_of_months, ab, bb, cb)\n",
    "\n",
    "# do the same for the temperature in the year 2100\n",
    "#temperature_in_2100_updated = .....\n",
    "\n",
    "print(f\"The expected global temperature in the year 2050 is {temperature_in_2050_updated} degrees C higher than the baseline\")\n",
    "print(f\"and the expected global temperature in the year 2100 is {temperature_in_2100_updated} degrees C higher than the baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ygCBPKKqcRgy",
   "metadata": {
    "id": "ygCBPKKqcRgy"
   },
   "source": [
    "# *Assignment 4, continued*\n",
    "\n",
    "**Compare the predicted temperatures in 2050 and 2100 with global temperature predicted by the The Intergovernmental Panel on Climate Change (IPCC) latest climate prediction, and write a few sentences on how your prediction compares to theirs, and to which emission scenario your prediction matches best. You can find the latest synthesis report here: https://www.ipcc.ch/report/ar6/syr/ , in the \"*Long Report*\" on page 65.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "klkdiipyeSEC",
   "metadata": {
    "id": "klkdiipyeSEC"
   },
   "source": [
    "*Add your answer here....*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7dc44e-9b2e-465c-85a0-96299282d9c4",
   "metadata": {
    "id": "af7dc44e-9b2e-465c-85a0-96299282d9c4"
   },
   "source": [
    "---\n",
    "# Section 7: Data analysis: rhythms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tsk5A8B3WShD",
   "metadata": {
    "id": "Tsk5A8B3WShD"
   },
   "source": [
    "In the preceding part of the exercise we looked at long-term trends in the data and how to best capture these with statistical models.\n",
    "\n",
    "Next we will look at *rhythms* or periodic changes in global climate. We will use a mathematical tool called [Fourier analysis](https://en.wikipedia.org/wiki/Fourier_analysis), which was invented by the French physicist Fourier. In short Fourier analysis decomposes a complex signal like global climate into a series of sine waves. These sine waves have a different ampltiude and frequency, and if there are any dominant periodic changes in global climate they will show up as a particular sine wave with a high amplitude.\n",
    "\n",
    "If you want to learn more about Fourier analysis, this video is highly recommended: https://www.youtube.com/watch?v=spUNpyF58BY\n",
    "\n",
    "For the Fourier analysis we will once more use the Python module scipy to do the Fourier analysis for us, using the fast Fourier transform library: https://docs.scipy.org/doc/scipy/reference/fft.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x_MHEk-MHRaA",
   "metadata": {
    "id": "x_MHEk-MHRaA"
   },
   "source": [
    "## 7.1 Detrending the data\n",
    "Before we start looking at periodic changes in the data, we first we need to detrend the temperature data to make it ready for the Fourier analysis. In the code below we calculate the detrended temperature curve by first calculating a [moving average](https://en.wikipedia.org/wiki/Moving_average), and then substracting this moving average from the actual temperature. A moving average is calculated by taking the average from all the neighboring values within a certain distance. In this case we will case a moving average in a 5 year window, using the [filter1d function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.uniform_filter1d.html) that was originally intended for image processing, but that we can also use to calculate a moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290ceb7-b5dd-48f6-a2a9-747ae0eaf32f",
   "metadata": {
    "id": "7290ceb7-b5dd-48f6-a2a9-747ae0eaf32f"
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "# size of the moving average window in months\n",
    "n_ma = 12 * 5\n",
    "\n",
    "# calculate the moving average\n",
    "df[\"moving_average_temperature\"] = uniform_filter1d(df[\"monthly_anomaly\"].values, size=n_ma)\n",
    "\n",
    "# calculate detrended temperatures\n",
    "df[\"detrended_temperature\"] = df[\"monthly_anomaly\"] - df[\"moving_average_temperature\"]\n",
    "\n",
    "# make a figure\n",
    "fig, ax = temperature_figure(df[\"date\"], [df[\"monthly_anomaly\"], df[\"moving_average_temperature\"], df[\"detrended_temperature\"]],\n",
    " [\"data\", \"moving average\", \"detrended data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tbDp4HW3ic30",
   "metadata": {
    "id": "tbDp4HW3ic30"
   },
   "source": [
    "## 7.2 Fourier analysis\n",
    "\n",
    "Now that we have removed the long-terms trends from the dataset, we can move on to the Fourier analysis of the detrended temperature data. Run the code block below to perform the Fourier analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea74ab-d5ee-47f4-983c-0b3f68aefe4f",
   "metadata": {
    "id": "7dea74ab-d5ee-47f4-983c-0b3f68aefe4f"
   },
   "outputs": [],
   "source": [
    "import scipy.fft as fft\n",
    "\n",
    "# define the length of one timestep, which is 1/12 year:\n",
    "T = 1.0 / 12.0\n",
    "#N = len(y)\n",
    "\n",
    "# get the number of datapoints\n",
    "N = len(y)\n",
    "\n",
    "# perform the Fourier analysis\n",
    "yf = fft.rfft(df[\"detrended_temperature\"].values, n=N)#[:N//2]\n",
    "\n",
    "# the fourier analysis returns a set of complex numbers.\n",
    "# Here we only use the real part of the complex number\n",
    "yfr = np.abs(yf)\n",
    "\n",
    "# get the frequencies\n",
    "xf = fft.rfftfreq(N, T)#[:N//2]\n",
    "\n",
    "# calculate the period from the frequency\n",
    "xp = 1.0 / xf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qc9vj2dHHclW",
   "metadata": {
    "id": "qc9vj2dHHclW"
   },
   "source": [
    "## 7.3 Visualizing the results of the Fourier analysis\n",
    "\n",
    "Next we will visualize the results of the Fourier analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454932c8-f4cc-4bfa-b9ac-8dc95ee38b3f",
   "metadata": {
    "id": "454932c8-f4cc-4bfa-b9ac-8dc95ee38b3f"
   },
   "outputs": [],
   "source": [
    "# only select periods >6 months and < 50 years\n",
    "xmin, xmax = 0.5, 50.0\n",
    "ind = (xp > 0.5) & (xp < 30.0)\n",
    "\n",
    "# create a figure\n",
    "fig, ax = pl.subplots(1, 1)\n",
    "\n",
    "# plot the frequencies\n",
    "ax.stem(xp[ind], yfr[ind])\n",
    "\n",
    "#ax.set_xscale(\"log\")\n",
    "# log scale for the y-axis\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "# set the limits of the x-axis\n",
    "ax.set_ylim(5, 100)\n",
    "\n",
    "# uncomment the line below for a logarithmic x-axis:\n",
    "#ax.set_xscale(\"log\")\n",
    "\n",
    "# add labels\n",
    "ax.set_xlabel(\"Period (year)\")\n",
    "ax.set_ylabel(\"Amplitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GfuTPosXQiHn",
   "metadata": {
    "id": "GfuTPosXQiHn"
   },
   "source": [
    "## 7.4 Visualizing the components of the climate signal\n",
    "\n",
    "The result above seems rather abstract and a bit difficult to interpret. It shows frequency on the x-axis and a measure of amplitude on the y-axis. However we can visualize how the Fourier analysis works by inverting the analysis and showing the sine wave for one or more of the frequencies or periods in the Fourier analysis. This is done in the code below. Try to play around with different numbers for the `frequencies_to_plot` variable to get a feeling for how the Fourier analysis works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e3acb-89b4-456f-9b29-56fcd5ccdf91",
   "metadata": {
    "id": "bf5e3acb-89b4-456f-9b29-56fcd5ccdf91"
   },
   "outputs": [],
   "source": [
    "# frequencies to show in the plot\n",
    "frequencies_to_plot = [2, 5, 10, 20]\n",
    "\n",
    "# generate a list to store the results of the inverse fourier model\n",
    "y_inverse_parts = []\n",
    "\n",
    "# go through all frequencies that we want to show in the figure\n",
    "for freq in frequencies_to_plot:\n",
    "  # calculate the inverse fourier transform for the first x frequencies:\n",
    "  y_inverse_part = fft.irfft(yf[:freq], n=N)\n",
    "  # add the inverse fourier transofrm to a list\n",
    "  y_inverse_parts.append(y_inverse_part)\n",
    "\n",
    "\n",
    "fig, ax = pl.subplots(1, 1)\n",
    "ax.plot(df[\"date\"], df[\"detrended_temperature\"] , lw=0.5, label=\"data\", color=\"gray\")\n",
    "#ax.plot(x, y_inverse, ls=\":\", lw=0.5)\n",
    "\n",
    "for freq, y_inverse_part in zip(frequencies_to_plot, y_inverse_parts):\n",
    "  label = f\"first {freq} frequencies\"\n",
    "  ax.plot(df[\"date\"], y_inverse_part, label=label, lw=1.5, zorder=10)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"Period (year)\")\n",
    "ax.set_ylabel(\"Amplitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y71ATyVXYuI0",
   "metadata": {
    "id": "Y71ATyVXYuI0"
   },
   "source": [
    "---\n",
    "# *Assignment 5:*\n",
    "\n",
    "The period vs amplitude figure above shows the period and the amplitude of each wave component that could be found in the detrended temperature signal.\n",
    "\n",
    "The most prominent cyclical change in global climate that we may expect to see in our data is the [El Nino Southern Oscillation (ENSO)](https://en.wikipedia.org/wiki/El_Ni%C3%B1o%E2%80%93Southern_Oscillation). However, this oscillation occurs every 2 to 7 years, and is therefore probably too irregular to be picked up as a clear peak in our Fourier analysis. In addition, El Nino results in warming in some areas and cooling in other, and may therefore not leave a clear global trace.\n",
    "\n",
    "Arguably the only clear signal visible in the data is a peak at one year. If you want you can visualize this better by setting the horizontal axis to logarithmic scale in the code block above.\n",
    "\n",
    "**Can you provide a reason why there is a one-year period visible in global temperatures? Would global temperature not be expected to be steady, given that summer in the northern hemisphere is compensated by winter in the southern hemisphere and vice-versa?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Sied0LCg8oc",
   "metadata": {
    "id": "1Sied0LCg8oc"
   },
   "source": [
    "*Enter your answer to assignment 5 here....*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ptWAvZEegRY6",
   "metadata": {
    "id": "ptWAvZEegRY6"
   },
   "source": [
    "---\n",
    "# Section 7: Global temperature and $CO_2$\n",
    "\n",
    "For the final part of this exercise we will look at what causes part of the temperature trend that we have analysed above. For this we will have a look at a global dataset of $CO_2$ in the atmosphere.  \n",
    "\n",
    "The best data on $CO_2$ in the atmosphere are from the National Oceanic and Atmospheric Administration (NOAA), who have measured $CO_2$ in the atmosphere from atop a volcano in Hawaii since the 1960's: https://gml.noaa.gov/ccgg/trends/\n",
    "\n",
    "They have also compiled data from several station globally to calculate the global average $CO_2$ concentration since the 1980s, which can be viewed here: https://gml.noaa.gov/ccgg/trends/global.html\n",
    " and the data can be downloaded here: https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_gl.csv\n",
    "\n",
    "We will use this dataset to explore the relation between global temperature and $CO_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wPPk4XmB-YUX",
   "metadata": {
    "id": "wPPk4XmB-YUX"
   },
   "source": [
    "## Obtaining the global atmospheric $CO_2$ data\n",
    "\n",
    "We start by downloading the data and having a look at its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hfat5kGTuIsb",
   "metadata": {
    "id": "Hfat5kGTuIsb"
   },
   "outputs": [],
   "source": [
    "# the location of the temperature data file\n",
    "fnc = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_gl.csv\"\n",
    "\n",
    "# read the datafile\n",
    "dfc = pd.read_csv(fnc, skiprows=38)\n",
    "\n",
    "# display teh contents of the file\n",
    "dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qqQDzf7xpW-y",
   "metadata": {
    "id": "qqQDzf7xpW-y"
   },
   "source": [
    "Next we construct a new column in the datafile that contains the date for each measurements, similarly to what we have done for the temperature data file earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4kGuMKX5TAfu",
   "metadata": {
    "id": "4kGuMKX5TAfu"
   },
   "outputs": [],
   "source": [
    "# determine the number of months in the entire dataset\n",
    "n_months = len(dfc)\n",
    "\n",
    "# find first year and the first months:\n",
    "# the first row of a dataframe can be called by df.iloc[0]\n",
    "first_year = int(dfc.iloc[0][\"year\"])\n",
    "first_month = int(dfc.iloc[0][\"month\"])\n",
    "\n",
    "# create a date range from the first month to the last\n",
    "dti = pd.date_range(f\"{first_year}-{first_month}-01\", periods=n_months, freq=\"MS\")\n",
    "\n",
    "# add a column with this date range to the dataframe\n",
    "dfc[\"date\"] = dti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1obIx5n759jO",
   "metadata": {
    "id": "1obIx5n759jO"
   },
   "source": [
    "## Visualizing global $CO_2$\n",
    "\n",
    "Now lets visualize the $CO_2$ data together with the temperature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QA6SWlDR2HW0",
   "metadata": {
    "id": "QA6SWlDR2HW0"
   },
   "outputs": [],
   "source": [
    "fig, ax = temperature_figure(df[\"date\"],\n",
    " [df[\"monthly_anomaly\"],\n",
    "  df[\"temperature_linear_model\"],\n",
    "  df[\"temperature_piecewise_linear_model\"],\n",
    "  df[\"temperature_powerlaw_model\"]],\n",
    " [\"data\", \"linear regression\", \"piecewise linear function\", \"powerlaw model\"])\n",
    "\n",
    "axr = ax.twinx()\n",
    "\n",
    "axr.plot(dfc[\"date\"], dfc[\"average\"], color=\"black\", label=r\"$CO_2$\")\n",
    "\n",
    "axr.set_ylabel(r\"$CO_2$ (ppm)\")\n",
    "\n",
    "axr.set_ylim(225, 475)\n",
    "\n",
    "axr.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rzH9J-C66EMe",
   "metadata": {
    "id": "rzH9J-C66EMe"
   },
   "source": [
    "## The relation between $CO_2$ and temperature\n",
    "\n",
    "From the figure above it seems that the increase in temperature since 1980 coincides with an increase in $CO_2$. Lets have a closer look at the relation between $CO_2$ and temperature by plotting these two variables against each other. Before we can do this we first have to merge the two datasets that contain the temperature data (`df`) and the $CO_2$ data (`dfc`). Below we merge these two datasets into a new one that we call `dft`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HwNEn9C5UIyF",
   "metadata": {
    "id": "HwNEn9C5UIyF"
   },
   "outputs": [],
   "source": [
    "# create a new column\n",
    "dfc[\"average_CO2\"] = dfc[\"average\"]\n",
    "\n",
    "# merge the temperature and CO2 datasets into a new dataset\n",
    "dft = df.merge(dfc, on=\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3paiPFYh6SXx",
   "metadata": {
    "id": "3paiPFYh6SXx"
   },
   "source": [
    "next, lets visualize the relation between global temperature and $CO_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aH2U374RUk3N",
   "metadata": {
    "id": "aH2U374RUk3N"
   },
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(1, 1)\n",
    "\n",
    "ax.scatter(dft[\"average_CO2\"], dft[\"monthly_anomaly\"])\n",
    "\n",
    "ax.set_xlabel(r\"$CO_2$ (ppm)\")\n",
    "ax.set_ylabel(r\"Temperature anomaly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oZo4dh0j8tcb",
   "metadata": {
    "id": "oZo4dh0j8tcb"
   },
   "source": [
    "## Quantifying the relation between global $CO_2$ and temperature\n",
    "\n",
    "The figure above show a positive correlation between temperature and $CO_2$. This is of course not entirely unexpected, because the greenhouse properties of $CO_2$ have already been known since 1896, as documented by [an article by the Swedish scientist Svante Arrhenius](https://www.tandfonline.com/doi/epdf/10.1080/14786449608620846?needAccess=true). Let's see if we can quantify this relation a bit better by running a linear regression between $CO_2$ and temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L3OxkejgU8ng",
   "metadata": {
    "id": "L3OxkejgU8ng"
   },
   "outputs": [],
   "source": [
    "# perform a linear regression\n",
    "regr_CO2 = st.linregress(dft[\"average_CO2\"].values, dft[\"monthly_anomaly\"].values)\n",
    "\n",
    "# display the results\n",
    "print(regr_CO2)\n",
    "\n",
    "# calculate the regression line\n",
    "rlx = np.linspace(dft[\"average_CO2\"].min(), dft[\"average_CO2\"].max(), 101)\n",
    "rly = rlx * regr_CO2.slope + regr_CO2.intercept\n",
    "\n",
    "# make a new figure\n",
    "fig, ax = pl.subplots(1, 1)\n",
    "\n",
    "# show the data\n",
    "ax.scatter(dft[\"average_CO2\"], dft[\"monthly_anomaly\"])\n",
    "\n",
    "# add the regression line\n",
    "ax.plot(rlx, rly, color=\"black\", lw=1.5)\n",
    "\n",
    "ax.set_xlabel(r\"$CO_2$ (ppm)\")\n",
    "ax.set_ylabel(r\"Temperature anomaly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7OyMfcSX_JZQ",
   "metadata": {
    "id": "7OyMfcSX_JZQ"
   },
   "source": [
    "Next we will use this linear relation to make a new model of global temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jUfgkwrqG2qI",
   "metadata": {
    "id": "jUfgkwrqG2qI"
   },
   "outputs": [],
   "source": [
    "# calculate global temperature based on the linear regression with CO2\n",
    "dft[\"temperature_CO2_model\"] = dft[\"average_CO2\"] * regr_CO2.slope + regr_CO2.intercept\n",
    "\n",
    "# calculate the R2 for the CO2 model\n",
    "R2_CO2 = sklearn.metrics.r2_score(dft[\"monthly_anomaly\"], dft[\"temperature_CO2_model\"])\n",
    "print(f\"R2 CO2 model = {R2_CO2}\")\n",
    "\n",
    "\n",
    "# create a figure\n",
    "fig, ax = temperature_figure(df[\"date\"],\n",
    " [df[\"monthly_anomaly\"],\n",
    "  df[\"temperature_linear_model\"],\n",
    "  df[\"temperature_piecewise_linear_model\"],\n",
    "  df[\"temperature_powerlaw_model\"]],\n",
    " [\"data\", \"linear regression\", \"piecewise linear function\", \"powerlaw model\"])\n",
    "\n",
    "# add our new temperate model\n",
    "ax.plot(dft[\"date\"], dft[\"temperature_CO2_model\"], color=\"black\", label=r\"$CO_2$ model\")\n",
    "\n",
    "# update the legend:\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tr03FYWfClt9",
   "metadata": {
    "id": "Tr03FYWfClt9"
   },
   "source": [
    "---\n",
    "\n",
    "## *Assignment 6 Evaluating our new global temperature model*\n",
    "\n",
    "Our new global temperature model looks quite good, but let's quantify how well it does compared to our earlier models by again calculating the AIC and BIC metrics as we have done before.\n",
    "\n",
    "**Complete the code block below to insert the AIC and BIC metric calculation for all the models that we have tried before. However this time use the merged dataframe `dft` instead of the temperature only dataframe `df` for the tests.**\n",
    "\n",
    "The reason we need to do this is that the CO2 model only runs from 1980 onward. And comparing its performance with the other models would be unfair if the other models are evaluated against a much longer temperature records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2oc4p6iGCpFS",
   "metadata": {
    "id": "2oc4p6iGCpFS"
   },
   "outputs": [],
   "source": [
    "AIC_CO2= AIC(dft[\"monthly_anomaly\"].values, dft[\"temperature_CO2_model\"].values, 2)\n",
    "BIC_CO2= BIC(dft[\"monthly_anomaly\"].values, dft[\"temperature_CO2_model\"].values, 2)\n",
    "print(f\"AIC for the CO2 model: {AIC_CO2}\")\n",
    "print(f\"BIC for the CO2 model: {BIC_CO2}\")\n",
    "\n",
    "# add the AIC and BIC for the other models here by copy-pasting code from\n",
    "# elsewhere in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9F3ZzqoDHMTc",
   "metadata": {
    "id": "9F3ZzqoDHMTc"
   },
   "source": [
    "## *Assignment 6 , continued:\n",
    "\n",
    "**Next write a few sentences on which model you prefer after this new evaluation and why you prefer this new model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835r6zFHVSc",
   "metadata": {
    "id": "8835r6zFHVSc"
   },
   "source": [
    "*Add your answer here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C7hg4ksaNhLi",
   "metadata": {
    "id": "C7hg4ksaNhLi"
   },
   "source": [
    "---\n",
    "\n",
    "# *Assignment 7: Climate sensitivity*\n",
    "\n",
    "In the analysis of CO2 and temperature you have actually calculated an important metric of climate change called *climate sensitivity*. This is the increase in temperature expected for a doubling of atmospheric $CO_2$ from the pre-industrial level of 280 ppm. **Write down your estimate of climate sensitivity below and discuss briefly how your answer compares to the climate sensitivty reported in the latest IPCC report.** This can be found in the latest technical summary report, on page 94, https://www.ipcc.ch/report/ar6/wg1/chapter/technical-summary/.\n",
    "\n",
    "*Hint: you can use the linear regression line of CO2 vs temperature that you completed earlier in section 7 to calculate climate sensitivty*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bMZLxeE7Of6r",
   "metadata": {
    "id": "bMZLxeE7Of6r"
   },
   "source": [
    "*Add your answer here....*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zjUh-o_SHZhj",
   "metadata": {
    "id": "zjUh-o_SHZhj"
   },
   "source": [
    "---\n",
    "\n",
    "# Done\n",
    "\n",
    "Congrats, you finished the first exercise! We hope you enjoyed it. In this exercise you learned basic data visualisation and data analysis skills for timeseries data. In the process you have also learned something about historic climate change and $CO_2$. Now you are ready to move on to working with climate data that goes much further back in time and that covers the last 65 million years of our planet's history. This will be the topic of next week's exercise."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
